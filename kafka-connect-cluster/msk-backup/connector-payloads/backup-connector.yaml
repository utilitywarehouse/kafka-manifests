---
name: msk-s3-backup-sink-all-parquet
config:
  name: msk-s3-backup-sink-all-parquet
  connector.class: io.lenses.streamreactor.connect.aws.s3.sink.S3SinkConnector
  tasks.max: '2'
  # include all topic, except:
  # - all topics created by mirror maker
  # - auth.iam-cerbos-audit-v1 currently this one can not be indexed due to an issue with the headers
  topics.regex: ^(?!auth\.iam-cerbos-audit-v1$)(?!__amazon_msk_canary$)(?!.*heartbeats$)(?!.*\.checkpoints\.internal$)(?!mm2-).*
  key.converter.schemas.enable: 'false'
  # consume from all topics, thus we need to specify the topic in the partitioning part.
  # using the Parquet format, as this is the most performant both for backup and restoring
  # 'store.envelope'=true - so that we can restore headers, timestamp, etc
  # 'partition.include.keys'=false - so that we get a clean folder structure. Example of written file: msk-backup-parquet/pubsub.examples/2/pubsub.examples(2_000010811937).parquet
  connect.s3.kcql: >-
    insert into `uw-dev-pubsub-msk-backup:msk-backup-parquet` 
    select * from `*` 
    PARTITIONBY _topic, _partition STOREAS `PARQUET` PROPERTIES (
    'store.envelope'=true,
    'flush.count'=10000,
    'flush.interval'=1800,
    'partition.include.keys'=false);
  value.converter.schemas.enable: 'false'
  connect.s3.aws.auth.mode: Default
  connect.s3.compression.codec: zstd
  connect.s3.compression.level: '9'
  connect.s3.aws.region: eu-west-1
  value.converter: org.apache.kafka.connect.converters.ByteArrayConverter
  key.converter: org.apache.kafka.connect.converters.ByteArrayConverter
